\documentclass[11pt,a4paper]{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{imakeidx}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{color}
\usepackage{url}
\usepackage{fancyhdr}
\usepackage{geometry}
\usepackage{xcolor}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{array}

% Index configuration
\makeindex[columns=1, title=Index, intoc]
\indexsetup{level=\section*,toclevel=section}
\makeatletter
\patchcmd{\theindex}{\section*{\indexname}}{\section*{\indexname}\indexmarkup}{}{}
\makeatother

% Custom index entries with proper formatting
\newcommand{\indexentry}[2]{\index{#1@\texttt{#1} #2}}
\newcommand{\indexfunc}[1]{\indexentry{#1}{function}}
\newcommand{\indexparam}[1]{\indexentry{#1}{parameter}}
\newcommand{\indexopt}[1]{\indexentry{#1}{option}}
\newcommand{\indexconst}[1]{\indexentry{#1}{constant}}
\newcommand{\indexmod}[1]{\indexentry{#1}{module}}

% Command for code in text
\newcommand{\code}[1]{\texttt{#1}}

% Command for important notes
\newcommand{\important}[1]{\textbf{#1}}

% Command for configuration options
\newcommand{\configopt}[1]{\texttt{#1}}

% Command for function signatures
\newcommand{\func}[1]{\texttt{#1}}

% Command for module names
\newcommand{\modname}[1]{\texttt{#1}}

% Page layout
\geometry{left=2cm,right=2cm,top=2.5cm,bottom=2.5cm}
\setlength{\headheight}{14pt}
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt}

% Headers and footers
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\leftmark}
\fancyhead[R]{\thepage}
\fancyfoot[C]{\thepage}

% Define Julia language for listings
\lstdefinelanguage{Julia}{
    morekeywords={abstract,break,case,catch,continue,do,else,elseif,end,export,for,function,immutable,import,if,in,macro,module,otherwise,quote,return,switch,type,typealias,using,while},
    morecomment=[l]{\#},
    morecomment=[n]{\#=}{=\#},
    morestring=[s]{"}{"},
    morestring=[m]{'}{'}
}

% Custom colors
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\definecolor{lightgray}{rgb}{0.95,0.95,0.95}

% Listings style
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize\ttfamily,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2,
    language=Julia,
    frame=single,
    rulecolor=\color{gray!30}
}

\lstset{style=mystyle}

% Custom commands
% Define custom commands if they don't already exist
\providecommand{\code}[1]{\texttt{\color{blue!70!black}#1}}
\providecommand{\important}[1]{\textbf{\color{red!70!black}#1}}

% Title and author
\title{OscarAICoder.jl Documentation}
\author{Author Name}
\date{\today}

\begin{document}
\maketitle
\thispagestyle{empty}

\begin{abstract}
This document provides comprehensive documentation for the \modname{OscarAICoder.jl} package, a powerful tool for generating Oscar code from natural language mathematical statements. The package supports multiple backends, context management, and various configuration options to customize the code generation process.
\end{abstract}

\section{Introduction}
\label{sec:introduction}

\modname{OscarAICoder.jl} is a Julia package that provides an interface to interact with large language models (LLMs) for generating Oscar code from natural language mathematical statements. The package is designed to be flexible, supporting multiple backends and configuration options to suit various use cases.

\subsection{Features}
\begin{itemize}
    \item Multiple backend support (local Ollama, Hugging Face, GitHub)
    \item Context-aware code generation
    \item Dictionary-based statement processing
    \item Debug and training modes
    \item Session management and persistence
    \item Customizable configuration options
\end{itemize}

\subsection{Overview}
The package consists of several modules that work together to provide a seamless experience for generating Oscar code. The main components are:

\begin{description}
    \item[\modname{OscarAICoder.jl}] Main module that provides the user-facing API
    \item[\modname{Backends}] Handles communication with different LLM backends
    \item[\modname{SeedDictionary}] Contains predefined statement-to-code mappings
    \item[\modname{Execute}] Manages code execution and result formatting
\end{description}

\section{Quick Start}
\label{sec:quickstart}

\subsection{Basic Installation}
\label{sec:installation}

To install \modname{OscarAICoder.jl}, use the Julia package manager:

\begin{lstlisting}
using Pkg
Pkg.add(path="/path/to/OscarAICoder")
\end{lstlisting}

\subsection{Basic Usage}
\label{sec:basicusage}

\subsubsection{Processing a Statement}
To convert a mathematical statement to Oscar code:

\begin{lstlisting}
using OscarAICoder

# Process a mathematical statement
oscar_code = process_statement("Find the roots of x^2 - 4x + 4")
\end{lstlisting}

\subsubsection{Executing Code}
To execute the generated Oscar code:

\begin{lstlisting}
# Execute the code
result = execute_statement(oscar_code)
\end{lstlisting}

\subsubsection{Using History}
To execute the most recent statement from history:

\begin{lstlisting}
# Execute the most recent statement
result = execute_all_statements()
\end{lstlisting}

\section{Installation}
\label{sec:installation_details}

\subsection{Basic Requirements}
\begin{itemize}
    \item Julia 1.6 or later
    \item Oscar.jl package
    \item HTTP.jl and JSON.jl packages
\end{itemize}

\subsection{Setting Up LLM Backends}

\subsubsection{Local LLM Server (Recommended)}
1. Install Ollama:
\begin{lstlisting}
wget https://raw.githubusercontent.com/wagh/OscarAICoder.jl/main/setup_ollama.sh
chmod +x setup_ollama.sh
./setup_ollama.sh
\end{lstlisting}

2. After installation:
\begin{lstlisting}
using OscarAICoder
oscar_code = process_statement("Factor the polynomial x^2 - 5x + 6 over the integers.")
\end{lstlisting}

\subsubsection{Remote Backends}
\begin{itemize}
    \item Hugging Face
    \item GitHub
    \item Remote Ollama server
\end{itemize}

\subsection{Configuration}
\begin{itemize}
    \item Backend settings
    \item Dictionary mode
    \item Debug mode
    \item Context management
\end{itemize}

\section{Support}
\label{sec:support}

\subsection{Documentation}
For detailed documentation, including:
\begin{itemize}
    \item Mathematical examples
    \item Advanced usage
    \item Configuration options
    \item Error handling
    \item Validation rules
    \item Backend-specific configurations
\end{itemize}

\subsection{Getting Help}
\begin{itemize}
    \item Check the documentation first
    \item Open an issue on the repository for bugs and feature requests
    \item For usage questions, check the examples section
    \item For technical support, contact the maintainers
\end{itemize}

\subsection{Common Issues}
\begin{itemize}
    \item Backend connection issues
    \item Invalid Oscar code generation
    \item Context management problems
    \item Configuration errors
\end{itemize}



\section{Basic Usage}
\label{sec:basic_usage}

\subsection{Processing Statements}

The main function to process natural language statements is \func{process\_statement}:

\begin{lstlisting}[language=Julia]
using OscarAICoder

# Process a simple statement
code = process_statement("Find the roots of x^2 - 5x + 6")
\end{lstlisting}

\subsection{Backend Configuration}

\modname{OscarAICoder.jl} supports multiple backends for processing statements. The default backend can be configured using:

\begin{lstlisting}[language=Julia]
# Configure the default backend
configure_default_backend(:local)  # Options: :local, :huggingface, :github

# Configure backend-specific settings
configure_local_backend(
    url="http://localhost:11434/api/generate",
    model="qwen2.5-coder"
)

configure_huggingface_backend(
    api_key="your_api_key",
    model="gpt2"
)

configure_github_backend(
    repo="username/repo",
    token="your_github_token"
)
\end{lstlisting}

\subsection{Context Management}

The package maintains conversation context to improve code generation. You can manage the context using:

\begin{lstlisting}[language=Julia]
# Get the current context
context = get_context()

# Clear the context
clear_context!()
\end{lstlisting}

\section{Advanced Features}
\label{sec:advanced-features}

\subsection{Backend Configuration}
\label{subsec:backend_config}

\modname{OscarAICoder.jl} supports multiple backends for processing statements. Each backend has its own configuration options:

\subsubsection{Local Backend}
The local backend connects to a locally running Ollama server.

\begin{lstlisting}[language=Julia]
# Configure local backend
configure_local_backend(
    url="http://localhost:11434/api/generate",
    model="qwen2.5-coder",
    temperature=0.7,
    max_tokens=256
)
\end{lstlisting}

\subsubsection{Hugging Face Backend}
The Hugging Face backend uses the Hugging Face Inference API.

\begin{lstlisting}[language=Julia]
# Configure Hugging Face backend
configure_huggingface_backend(
    api_key="your_api_key",
    model="gpt2",
    endpoint="https://api-inference.huggingface.co/models"
)
\end{lstlisting}

\subsubsection{GitHub Backend}
The GitHub backend interacts with GitHub's API for code generation.

\begin{lstlisting}[language=Julia]
# Configure GitHub backend
configure_github_backend(
    repo="username/repo",
    token="your_github_token",
    branch="main",
    model="llama2"
)
\end{lstlisting}

\subsection{Dictionary Mode}
\label{subsec:dictionary_mode}

The package includes a dictionary-based approach for common mathematical operations. You can configure the dictionary mode using:

\begin{lstlisting}[language=Julia]
# Configure dictionary mode
configure_dictionary_mode(:priority)  # Options: :priority, :only, :disabled
\end{lstlisting}

\begin{description}
    \item[\code{:priority}] (default) Check dictionary first, then use LLM
    \item[\code{:only}] Only use dictionary, don't use LLM
    \item[\code{:disabled}] Always use LLM, ignore dictionary
\end{description}

\subsection{Debug and Training Modes}

\begin{lstlisting}[language=Julia]
# Enable debug mode for detailed logging
debug_mode!(true)

# Enable training mode for more detailed prompts
training_mode!(true)
\end{lstlisting}

\section{Examples}
\label{sec:examples}

Here are some examples of using \modname{OscarAICoder.jl} for various mathematical tasks:

\subsection{Basic Algebra}

\begin{lstlisting}[language=Julia]
# Solve a quadratic equation
code = process_statement("Find the roots of x^2 - 5x + 6")
println(code)

# Expected output:
# R, x = PolynomialRing(QQ, "x")
# f = x^2 - 5x + 6
# roots(f)
\end{lstlisting}

\subsection{Linear Algebra}

\begin{lstlisting}[language=Julia]
# Compute eigenvalues of a matrix
code = process_statement("Find the eigenvalues of matrix [1 2; 3 4]")
println(code)

# Expected output:
# M = matrix(QQ, [1 2; 3 4])
# eigvals(M)
\end{lstlisting}

\subsection{Number Theory}

\begin{lstlisting}[language=Julia]
# Check if a number is prime
code = process_statement("Check if 17 is a prime number")
println(code)

# Expected output:
# is_prime(17)  # Returns true
\end{lstlisting}

\section{Troubleshooting}
\label{sec:troubleshooting}

\subsection{Common Issues}

\begin{description}
    \item[\important{LLM not responding}] Check if the LLM server is running and accessible. For local setups, ensure Ollama is properly installed and running.
    
    \item[\important{Context not maintained}] Ensure you're not clearing the context unintentionally and that the conversation history length is sufficient.
\end{description}

\subsection{Debugging}

You can enable debug mode to get more detailed information about the code generation process:

\begin{lstlisting}[language=Julia]
# Enable debug mode
debug_mode!(true)

# Process a statement to see debug output
process_statement("your statement")
\end{lstlisting}

\section{API Reference}
\label{sec:api_reference}

\subsection{Main Functions}

\begin{description}
    \item[\func{process\_statement(statement::String; kwargs...)}] \hfill \\
    Processes a natural language mathematical statement and returns the corresponding Oscar code.
    \indexfunc{process\_statement}
    
    \item[\func{configure\_default\_backend(backend::Symbol; kwargs...)}] \hfill \\
    Configures the default LLM backend with the specified parameters.
    \indexfunc{configure\_default\_backend}
    
    \item[\func{configure\_dictionary\_mode(mode::Symbol)}] \hfill \\
    Configures how the dictionary is used when processing statements.
    \indexfunc{configure\_dictionary\_mode}
    
    \item[\func{training\_mode!(enabled::Bool)}] \hfill \\
    Enables or disables training mode, which provides more detailed prompts to the LLM.
    \indexfunc{training\_mode!}
\end{description}

\subsection{Context Management}

\begin{description}
    \item[\func{get\_context()}] \hfill \\
    Returns the current conversation context.
    \indexfunc{get\_context}
    
    \item[\func{clear\_context!()}] \hfill \\
    Clears the current conversation context.
    \indexfunc{clear\_context!}
    
    \item[\func{save\_context(filename::String="")}] \hfill \\
    Saves the current context to a file.
    \indexfunc{save\_context}
    
    \item[\func{load\_context(filename::String; clear\_current::Bool=true)}] \hfill \\
    Loads a context from a file.
    \indexfunc{load\_context}
    
    \item[\func{set\_save\_dir(dir::String)}] \hfill \\
    Sets the directory where session files will be saved.
    \indexfunc{set\_save\_dir}
\end{description}

\subsection{Backend Functions}

\begin{description}
    \item[\func{configure\_local\_backend(;url, model, temperature, max\_tokens)}] \hfill \\
    Configures the local Ollama backend.
    \indexfunc{configure\_local\_backend}
    
    \item[\func{configure\_huggingface\_backend(;api\_key, model, endpoint)}] \hfill \\
    Configures the Hugging Face backend.
    \indexfunc{configure\_huggingface\_backend}
    
    \item[\func{configure\_github\_backend(;repo, token, branch, model)}] \hfill \\
    Configures the GitHub backend.
    \indexfunc{configure\_github\_backend}
\end{description}

\subsection{Debugging Functions}

\begin{description}
    \item[\func{debug\_mode!(state::Bool)}] \hfill \\
    Enables or disables debug mode for detailed logging.
    \indexfunc{debug\_mode!}
    
    \item[\func{debug\_mode()}] \hfill \\
    Returns the current debug mode state.
    \indexfunc{debug\_mode}
    
    \item[\func{debug\_print(msg::String; prefix="[DEBUG]"}] \hfill \\
    Prints a debug message if debug mode is enabled.
    \indexfunc{debug\_print}
\end{description}

\subsection{Execution Functions}

\begin{description}
    \item[\func{execute\_statement(oscar\_code::String; clear\_context=false)}] \hfill \\
    Executes Oscar code and returns the result.
    \indexfunc{execute\_statement}
    
    \item[\func{execute\_statement\_with\_format(oscar\_code::String; output\_format=:string)}] \hfill \\
    Executes Oscar code and returns the result in the specified format.
    \indexfunc{execute\_statement\_with\_format}
\end{description}

\section{Best Practices}
\label{sec:best_practices}

\begin{itemize}
    \item Always start with dictionary mode enabled for common operations
    \item Use training mode to improve the dictionary with new mathematical concepts
    \item Keep debug mode enabled during development for better insights
    \item Regularly update the dictionary with new mathematical patterns
\end{itemize}

\section{Testing}
\label{sec:testing}

The package includes a comprehensive test suite organized by mathematical area:

\begin{itemize}
    \item \texttt{doc/tst/commutative\_algebra/}
    \item \texttt{doc/tst/algebraic\_geometry/}
    \item \texttt{doc/tst/calculus/}
    \item \texttt{doc/tst/linear\_algebra/}
\end{itemize}

To run the tests, use the \texttt{make test} command:

\begin{lstlisting}[language=bash]
make test
\end{lstlisting}

\section{Contributing}
\label{sec:contributing}
\index{Contributing}

Contributions to OscarAICoder.jl are welcome! Please see the \texttt{CONTRIBUTING.md} file for guidelines.

\section*{Acknowledgements}
\label{sec:acknowledgements}
\index{Acknowledgements}

We would like to express our gratitude to the following individuals and organizations for their contributions and support:

\begin{itemize}
    \item The OSCAR development team for creating and maintaining the OSCAR computer algebra system
    \item The Julia community for their support and contributions
    \item All contributors who have helped improve OscarAICoder.jl through bug reports, feature requests, and code contributions
    \item The developers of the various LLM backends that make this package possible
\end{itemize}

\section{License}
\label{sec:license}
\index{License}

OscarAICoder.jl is licensed under the MIT License.

\clearpage
\section*{Index}\addcontentsline{toc}{section}{Index}
\printindex

\end{document}

