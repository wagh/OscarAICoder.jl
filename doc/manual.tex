\documentclass[11pt,a4paper]{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{makeidx}
\usepackage{imakeidx}
\usepackage{listings}
\usepackage{color}
\usepackage{url}
\usepackage{fancyhdr}
\usepackage{geometry}
\usepackage{xcolor}
\usepackage{enumitem}
\usepackage{array}
\usepackage{hyperref}

% Index configuration
\makeindex% [columns=1, title=Index, intoc]
% \indexsetup{level=\section*,toclevel=section}
% \makeatletter
% \patchcmd{\theindex}{\section*{\indexname}}{\section*{\indexname}\indexmarkup}{}{}
% \makeatother

% Custom index entries with proper formatting
\newcommand{\indexentry}[2]{\index{#1@\texttt{#1} #2}}
\newcommand{\indexfunc}[1]{\indexentry{#1}{function}}
\newcommand{\indexparam}[1]{\indexentry{#1}{parameter}}
\newcommand{\indexopt}[1]{\indexentry{#1}{option}}
\newcommand{\indexconst}[1]{\indexentry{#1}{constant}}
\newcommand{\indexmod}[1]{\indexentry{#1}{module}}

% Index formatting
\renewcommand{\indexname}{Index}
\renewcommand{\indexspace}{\vspace{12pt}}
% \renewcommand{\indexfont}{\bfseries}

% Command for code in text
\newcommand{\code}[1]{\texttt{#1}}

% Command for important notes
\newcommand{\important}[1]{\textbf{#1}}

% Command for configuration options
\newcommand{\configopt}[1]{\texttt{#1}}

% Command for function signatures
\newcommand{\func}[1]{\texttt{#1}}

% Command for module names
\newcommand{\modname}[1]{\texttt{#1}}

% Page layout
\geometry{left=2cm,right=2cm,top=2.5cm,bottom=2.5cm}
\setlength{\headheight}{14pt}
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt}

% Headers and footers
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\leftmark}
\fancyhead[R]{\thepage}
\fancyfoot[C]{\thepage}

% Define Julia language for listings
\lstdefinelanguage{Julia}{
    morekeywords={abstract,break,case,catch,continue,do,else,elseif,end,export,for,function,immutable,import,if,in,macro,module,otherwise,quote,return,switch,type,typealias,using,while},
    morecomment=[l]{\#},
    morecomment=[n]{\#=}{=\#},
    morestring=[s]{"}{"},
    morestring=[m]{'}{'}
}

% Custom colors
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\definecolor{lightgray}{rgb}{0.95,0.95,0.95}

% Listings style
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize\ttfamily,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2,
    language=Julia,
    frame=single,
    rulecolor=\color{gray!30}
}

\lstset{style=mystyle}

% Custom commands
% Define custom commands if they don't already exist
\providecommand{\code}[1]{\texttt{\color{blue!70!black}#1}}
\providecommand{\important}[1]{\textbf{\color{red!70!black}#1}}

\title{OscarAICoder.jl Documentation}
\author{OscarAICoder Development Team}
\date{\today}

\begin{document}

\maketitle
\thispagestyle{empty}

\begin{abstract}
  This document provides comprehensive documentation for the
  \modname{OscarAICoder.jl} package, a powerful tool for generating Oscar code
  from natural language mathematical statements. The package supports multiple
  backends, context management, and various configuration options to customize
  the code generation process.
\end{abstract}

\section{Introduction}
\label{sec:introduction}

\modname{OscarAICoder.jl} is a Julia package that provides an interface to interact with large language models (LLMs) for generating Oscar code from natural language mathematical statements. The package is designed to be flexible, supporting multiple backends and configuration options to suit various use cases.

\section{Basic Usage}
\label{sec:basic_usage}

\subsection{Processing Statements}

The main function to process natural language statements is \func{process\_statement}:

\begin{lstlisting}
using OscarAICoder

# Process a mathematical statement
oscar_code = process_statement("Find the roots of x^2 - 4x + 4")

# Execute the code
result = execute_statement(oscar_code)

# View history
entries = History.get_entries()
\end{lstlisting}

\subsection{Using Different Backends}

The package supports multiple backends for code generation:

\begin{itemize}
    \item \texttt{LOCAL}: Uses local Ollama server
    \item \texttt{REMOTE}: Uses remote API
    \item \texttt{HUGGINGFACE}: Uses Hugging Face models
    \item \texttt{GITHUB}: Uses GitHub Copilot
\end{itemize}

\subsection{Dictionary Mode}

The package includes a dictionary mode for faster processing of common statements:

\begin{lstlisting}
# Enable dictionary mode
Config.CONFIG.dictionary_mode = :enabled

# Process common statements faster
oscar_code = process_statement("Find the roots of x^2 - 1")
\end{lstlisting}

\subsection{Debug Mode}

Debug mode provides detailed logging of the code generation process:

\begin{lstlisting}
# Enable debug mode
Config.CONFIG.debug = true

# See detailed logs
process_statement("Find the roots")
\end{lstlisting}

\section{Advanced Features}

\subsection{History Management}

The package maintains a history of generated code:

\begin{lstlisting}
# Add an entry to history
History.add_entry!("Find the roots", oscar_code)

# Get all entries
entries = History.get_entries()

# Clear history
History.clear!()
\end{lstlisting}

\subsection{Context Management}

Context management allows for better code generation by maintaining state:

\begin{lstlisting}
# Set context
Context.set("variable", value)

# Get context
value = Context.get("variable")
\end{lstlisting}

\section{Configuration}

The package can be configured through the \modname{Config} module:

\begin{lstlisting}
# Set default backend
Config.CONFIG.default_backend = REMOTE

# Set model
Config.CONFIG.model = "gpt-4"

# Enable training mode
Config.CONFIG.training_mode = true
\end{lstlisting}

\section{Validation}

The package includes validation to ensure generated code is correct:

\begin{lstlisting}
# Validate Oscar code
is_valid = validate_oscar_code(oscar_code)

# Get validation errors
errors = get_validation_errors(oscar_code)
\end{lstlisting}

\section{Support}
\label{sec:support}

\subsection{Documentation}
For detailed documentation, including:
\begin{itemize}
    \item Mathematical examples
    \item Advanced usage
    \item Configuration options
    \item Error handling
    \item Validation rules
    \item Backend-specific configurations
\end{itemize}

\subsection{Getting Help}
\begin{itemize}
    \item Check the documentation first
    \item Open an issue on the repository for bugs and feature requests
    \item For usage questions, check the examples section
    \item For technical support, contact the maintainers
\end{itemize}

\subsection{Common Issues}
\begin{itemize}
    \item Backend connection issues
    \item Invalid Oscar code generation
    \item Context management problems
    \item Configuration errors
\end{itemize}

\printindex

\section{Advanced Features}
\label{sec:advanced-features}

\subsection{Backend Configuration}
\label{subsec:backend_config}

\modname{OscarAICoder.jl} supports multiple backends for processing statements. Each backend has its own configuration options:

\subsubsection{Local Backend}
The local backend connects to a locally running Ollama server.

\begin{lstlisting}[language=Julia]
# Configure local backend
configure_local_backend(
    url="http://localhost:11434/api/generate",
    model="qwen2.5-coder",
    temperature=0.7,
    max_tokens=256
)
\end{lstlisting}

\subsubsection{Hugging Face Backend}
The Hugging Face backend uses the Hugging Face Inference API.

\begin{lstlisting}[language=Julia]
# Configure Hugging Face backend
configure_huggingface_backend(
    api_key="your_api_key",
    model="gpt2",
    endpoint="https://api-inference.huggingface.co/models"
)
\end{lstlisting}

\subsubsection{GitHub Backend}
The GitHub backend interacts with GitHub's API for code generation.

\begin{lstlisting}[language=Julia]
# Configure GitHub backend
configure_github_backend(
    repo="username/repo",
    token="your_github_token",
    branch="main",
    model="llama2"
)
\end{lstlisting}

\subsection{Dictionary Mode}
\label{subsec:dictionary_mode}

The package includes a dictionary-based approach for common mathematical operations. You can configure the dictionary mode using:

\begin{lstlisting}[language=Julia]
# Configure dictionary mode
configure_dictionary_mode(:priority)  # Options: :priority, :only, :disabled
\end{lstlisting}

\begin{description}
    \item[\code{:priority}] (default) Check dictionary first, then use LLM
    \item[\code{:only}] Only use dictionary, don't use LLM
    \item[\code{:disabled}] Always use LLM, ignore dictionary
\end{description}

\subsection{Debug and Training Modes}

\begin{lstlisting}[language=Julia]
# Enable debug mode for detailed logging
debug_mode!(true)

# Enable training mode for more detailed prompts
training_mode!(true)
\end{lstlisting}

\section{Examples}
\label{sec:examples}

Here are some examples of using \modname{OscarAICoder.jl} for various mathematical tasks:

\subsection{Basic Algebra}

\begin{lstlisting}[language=Julia]
# Solve a quadratic equation
code = process_statement("Find the roots of x^2 - 5x + 6")
println(code)

# Expected output:
# R, x = PolynomialRing(QQ, "x")
# f = x^2 - 5x + 6
# roots(f)
\end{lstlisting}

\subsection{Linear Algebra}

\begin{lstlisting}[language=Julia]
# Compute eigenvalues of a matrix
code = process_statement("Find the eigenvalues of matrix [1 2; 3 4]")
println(code)

# Expected output:
# M = matrix(QQ, [1 2; 3 4])
# eigvals(M)
\end{lstlisting}

\subsection{Number Theory}

\begin{lstlisting}[language=Julia]
# Check if a number is prime
code = process_statement("Check if 17 is a prime number")
println(code)

# Expected output:
# is_prime(17)  # Returns true
\end{lstlisting}

\section{Troubleshooting}
\label{sec:troubleshooting}

\subsection{Common Issues}

\begin{description}
    \item[\important{LLM not responding}] Check if the LLM server is running and accessible. For local setups, ensure Ollama is properly installed and running.
    
    \item[\important{Context not maintained}] Ensure you're not clearing the context unintentionally and that the conversation history length is sufficient.
\end{description}

\subsection{Debugging}

You can enable debug mode to get more detailed information about the code generation process:

\begin{lstlisting}[language=Julia]
# Enable debug mode
debug_mode!(true)

# Process a statement to see debug output
process_statement("your statement")
\end{lstlisting}

\section{API Reference}
\label{sec:api_reference}

\subsection{Main Functions}

\begin{description}
    \item[\func{process\_statement(statement::String; kwargs...)}] \hfill \\
    Processes a natural language mathematical statement and returns the corresponding Oscar code.
    \indexfunc{process\_statement}
    
    \item[\func{configure\_default\_backend(backend::Symbol; kwargs...)}] \hfill \\
    Configures the default LLM backend with the specified parameters.
    \indexfunc{configure\_default\_backend}
    
    \item[\func{configure\_dictionary\_mode(mode::Symbol)}] \hfill \\
    Configures how the dictionary is used when processing statements.
    \indexfunc{configure\_dictionary\_mode}
    
    \item[\func{training\_mode!(enabled::Bool)}] \hfill \\
    Enables or disables training mode, which provides more detailed prompts to the LLM.
    \indexfunc{training\_mode!}
\end{description}

\subsection{Context Management}

\begin{description}
    \item[\func{get\_context()}] \hfill \\
    Returns the current conversation context.
    \indexfunc{get\_context}
    
    \item[\func{clear\_context!()}] \hfill \\
    Clears the current conversation context.
    \indexfunc{clear\_context!}
    
    \item[\func{save\_context(filename::String="")}] \hfill \\
    Saves the current context to a file.
    \indexfunc{save\_context}
    
    \item[\func{load\_context(filename::String; clear\_current::Bool=true)}] \hfill \\
    Loads a context from a file.
    \indexfunc{load\_context}
    
    \item[\func{set\_save\_dir(dir::String)}] \hfill \\
    Sets the directory where session files will be saved.
    \indexfunc{set\_save\_dir}
\end{description}

\subsection{Backend Functions}

\begin{description}
    \item[\func{configure\_local\_backend(;url, model, temperature, max\_tokens)}] \hfill \\
    Configures the local Ollama backend.
    \indexfunc{configure\_local\_backend}
    
    \item[\func{configure\_huggingface\_backend(;api\_key, model, endpoint)}] \hfill \\
    Configures the Hugging Face backend.
    \indexfunc{configure\_huggingface\_backend}
    
    \item[\func{configure\_github\_backend(;repo, token, branch, model)}] \hfill \\
    Configures the GitHub backend.
    \indexfunc{configure\_github\_backend}
\end{description}

\subsection{Debugging Functions}

\begin{description}
    \item[\func{debug\_mode!(state::Bool)}] \hfill \\
    Enables or disables debug mode for detailed logging.
    \indexfunc{debug\_mode!}
    
    \item[\func{debug\_mode()}] \hfill \\
    Returns the current debug mode state.
    \indexfunc{debug\_mode}
    
    \item[\func{debug\_print(msg::String; prefix="[DEBUG]" )}] \hfill \\
    Prints a debug message if debug mode is enabled.
    \indexfunc{debug\_print}
\end{description}

\subsection{Execution Functions}

\begin{description}
    \item[\func{execute\_statement(oscar\_code::String; clear\_context=false)}] \hfill \\
    Executes Oscar code and returns the result.
    \indexfunc{execute\_statement}
    
    \item[\func{execute\_statement\_with\_format(oscar\_code::String; output\_format=:string)}] \hfill \\
    Executes Oscar code and returns the result in the specified format.
    \indexfunc{execute\_statement\_with\_format}
\end{description}

\section{Best Practices}
\label{sec:best_practices}

\begin{itemize}
    \item Always start with dictionary mode enabled for common operations
    \item Use training mode to improve the dictionary with new mathematical concepts
    \item Keep debug mode enabled during development for better insights
    \item Regularly update the dictionary with new mathematical patterns
\end{itemize}

\bibliographystyle{plain}
\bibliography{references}

\section{Testing}
\label{sec:testing}

The package includes a comprehensive test suite organized by mathematical area:

\begin{itemize}
    \item \texttt{doc/tst/commutative\_algebra/}
    \item \texttt{doc/tst/algebraic\_geometry/}
    \item \texttt{doc/tst/calculus/}
    \item \texttt{doc/tst/linear\_algebra/}
\end{itemize}

To run the tests, use the \texttt{make test} command:

\begin{lstlisting}[language=bash]
make test
\end{lstlisting}

\section{Contributing}
\label{sec:contributing}
\index{Contributing}

Contributions to OscarAICoder.jl are welcome! Please see the \texttt{CONTRIBUTING.md} file for guidelines.

\section*{Acknowledgements}
\label{sec:acknowledgements}
\index{Acknowledgements}

We would like to express our gratitude to the following individuals and organizations for their contributions and support:

\begin{itemize}
    \item The OSCAR development team for creating and maintaining the OSCAR computer algebra system
    \item The Julia community for their support and contributions
    \item All contributors who have helped improve OscarAICoder.jl through bug reports, feature requests, and code contributions
    \item The developers of the various LLM backends that make this package possible
\end{itemize}

\section{License}
\label{sec:license}
\index{License}

OscarAICoder.jl is licensed under the MIT License.

\clearpage
\section*{Index}\addcontentsline{toc}{section}{Index}
\printindex
\end{document}

